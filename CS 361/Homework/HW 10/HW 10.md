#### Problem 1

<img src="./HW 10.assets/image-20240420224320178.png" alt="image-20240420224320178" style="zoom:50%;" />

Seems like we get a perfect prediction, so if we eat a mushroom that is predicted to be edible, it's close to 0% that it is poisonous.



#### Problem 2

<img src="./HW 10.assets/image-20240420224457614.png" alt="image-20240420224457614" style="zoom:50%;" /><img src="./HW 10.assets/image-20240420224507889.png" alt="image-20240420224507889" style="zoom:50%;" />

Left is the decision tree, right is the random forest.

###### (a)

Random Forest performs better than a single Decision Tree because it  reduces overfitting by averaging multiple trees, each trained on  different data subsets. This ensemble method increases accuracy,  decreases the impact of outliers, and reduces variance without  significantly raising bias. By using bootstrapping and feature randomness, it decorrelates  individual trees, ensuring that errors are minimized when predictions  are averaged, leading to better generalization on new data. This makes  Random Forests more reliable and versatile for both classification and  regression tasks.

###### (b)

Increasing the depth of decision tree (set it to unlimited), doesn't seem to increase its performance.

In most cases, a single decision tree cannot outperform a Random Forest, even without depth limitations, due to increased overfitting risks in deeper trees. Random Forests mitigate this by averaging multiple trees, reducing variance and enhancing generalization. From this dataset, where a deeper decision tree still underperforms compared to Random Forest, demonstrating the advantages of ensemble learning over single-model approaches.



#### Problem 3

<img src="./HW 10.assets/image-20240420224709390.png" alt="image-20240420224709390" style="zoom:50%;" /><img src="./HW 10.assets/image-20240420224720120.png" alt="image-20240420224720120" style="zoom:50%;" />

Left: decision tree, Right: random forest

##### (a)

The Haberman's survival dataset may challenge decision trees and random forests due to its imbalanced classes (class 1 is bigger then class 2), limited size (only few hundred data), and potentially weak feature-target relationships (it seems that there are data with similar feature vector but some are class 1, some are class 2). Overfitting could result from the small dataset and noise within, while the algorithms might also struggle with complex patterns and outliers

##### (b)

Based on the confusion matrices, we cannot definitively  conclude that one model is superior to the other. Both models seem to have issues, possibly due to overfitting, class imbalance, or lack of feature relevance. However, we typically expect a random forest to perform better than a single  decision tree because it reduces variance by averaging multiple decision trees, thereby improving generalization. The decision tree might be  more prone to overfitting, especially with small datasets. (As we see that random forest is *slightly slightly* better than decision tree)



#### Problem 4

```
Decision Tree Classifier: 0.8359813084112149
Random Forest Classifier: 0.9298397863818424
K Nearest Neighbors Classifier: 0.9665554072096129
Gaussian Naive Bayes Classifier: 0.4615487316421897
```

K Nearest Neighbor seems to be the best.

<img src="./HW 10.assets/image-20240420235450975.png" alt="image-20240420235450975" style="zoom:50%;" />

It's accuracy is `0.9665554072096129`.

#### Code

```python
# %%
from ucimlrepo import fetch_ucirepo

# fetch dataset
mushroom = fetch_ucirepo(id=73)

# data (as pandas dataframes)
X = mushroom.data.features
y = mushroom.data.targets

# %%
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt
import random

# Example data loading
# X, y = pd.read_csv('your_dataset.csv').drop('target', axis=1), pd.read_csv('your_dataset.csv')['target']

# Identify categorical columns
categorical_columns = X.select_dtypes(include=['object']).columns

# Convert categorical columns to numeric using LabelEncoder
# label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    X.loc[:, col] = le.fit_transform(X[col])
    # label_encoders[col] = le


# %%
# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = random.randint(0, 100), train_size=0.8, test_size=0.2)

y_train = y_train.squeeze()
y_test = y_test.squeeze()

# Train the RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100, random_state=random.randint(0, 100), max_depth=50)
clf.fit(X_train, y_train)

# Predict the classes for the test set
y_pred = clf.predict(X_test)

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plotting the confusion matrix
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# %% [markdown]
# 

# %%
from ucimlrepo import fetch_ucirepo

# fetch dataset 
eeg_eye_state = fetch_ucirepo(id=264)

# data (as pandas dataframes)
X = eeg_eye_state.data.features
y = eeg_eye_state.data.targets

# %%
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
import sklearn.metrics as metrics
import sklearn.model_selection as ms

accuracy = metrics.make_scorer(metrics.accuracy_score)

y_flat = y.squeeze()

clf_score = cross_val_score(
    DecisionTreeClassifier(random_state=random.randint(0, 100), max_depth=50),
    X, y_flat, cv=ms.KFold(shuffle=True), scoring=accuracy
)
rf_clf_score = cross_val_score(
    RandomForestClassifier(random_state=random.randint(0, 100), n_estimators=100, max_depth=50),
    X, y_flat, cv=ms.KFold(shuffle=True), scoring=accuracy
)
knn_score = cross_val_score(
    KNeighborsClassifier(n_neighbors=5),
    X, y_flat, cv=ms.KFold(shuffle=True), scoring=accuracy
)
gnb_score = cross_val_score(
    GaussianNB(),
    X, y_flat, cv=ms.KFold(shuffle=True), scoring=accuracy
)

print(f"Decision Tree Classifier: {clf_score.mean()}")
print(f"Random Forest Classifier: {rf_clf_score.mean()}")
print(f"K Nearest Neighbors Classifier: {knn_score.mean()}")
print(f"Gaussian Naive Bayes Classifier: {gnb_score.mean()}")

# %%
from sklearn.tree import DecisionTreeClassifier  # For classification tasks

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random.randint(0, 100), train_size=0.8, test_size=0.2)

y_train = y_train.squeeze()
y_test = y_test.squeeze()

clf = DecisionTreeClassifier(random_state=random.randint(0, 100), max_depth=50)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='g')


# %%
# do the thing for random forest
from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(n_estimators=100, random_state=random.randint(0, 100), max_depth=50)

rf_clf.fit(X_train, y_train)

y_pred = rf_clf.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='g')

# %%
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='g')

# %%
from sklearn.naive_bayes import GaussianNB


# Create Gaussian Naive Bayes classifier
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# Predict and evaluate
y_pred = gnb.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='g')


# %%
# read haberman.data

haberman = pd.read_csv('haberman.data', header=None)

X = haberman.drop(3, axis=1)
y = haberman[3]

# %%
# do decision tree
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random.randint(0, 100), train_size=0.8, test_size=0.2)

y_train = y_train.squeeze()
y_test = y_test.squeeze()

clf = DecisionTreeClassifier(random_state=random.randint(0, 100), max_depth=50)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='g')

# %%
# do random forest

clf = RandomForestClassifier(n_estimators=100, random_state=random.randint(0, 100), max_depth=50)

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='g')

# %%
X = [[0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 0], [0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1], [0, 0, 0], [1, 1, 1]]
y = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]

score = cross_val_score(
    DecisionTreeClassifier(random_state=random.randint(0, 100), max_depth=50),
    X, y, cv=5, scoring='accuracy'
)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random.randint(0, 100), train_size=0.8, test_size=0.2)

clf = DecisionTreeClassifier(random_state=random.randint(0, 100), max_depth=50)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='g')

print(score.mean())
```

